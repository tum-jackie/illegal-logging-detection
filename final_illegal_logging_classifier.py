# -*- coding: utf-8 -*-
"""Final_Illegal_Logging_Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZZqBIbr8DsMiJCmLZe3NNYrBZvd0Y7nl
"""

!nvidia-smi

from google.colab import drive
drive.mount('/content/drive')

!unzip "/content/drive/MyDrive/new1.zip" -d "/content/"
#!unzip "/content/drive/MyDrive/extracted_features.zip" -d "/content/Extracted_features/"

from os import listdir
from os.path import isdir, join
from tensorflow.keras import layers, models
import numpy as np
import tensorflow as tf
from keras import metrics

# Create list of all targets for the LogNet dataset
dataset_path = '/content/LogNet V1.0'
lognet_dataset = all_targets = [name for name in listdir(dataset_path) if isdir(join(dataset_path, name))]
print(all_targets)

feature_sets_path = '/content/drive/MyDrive/Demo.zip'
feature_sets_filename = '/content/extracted_features.npz'
model_filename = 'chainsaw_detection_model.h5'
alert = 'Chainsaw'

# Load extracted feature sets from the LogNet Dataset
feature_sets = np.load(join(feature_sets_path, feature_sets_filename))
print(feature_sets.files)

# Assign feature sets for training, validation and testing
x_train = feature_sets['x_train']
y_train = feature_sets['y_train']
x_val = feature_sets['x_val']
y_val = feature_sets['y_val']
x_test = feature_sets['x_test']
y_test = feature_sets['y_test']

#Tensor dimensions for the training data
print(x_train.shape)
print(x_val.shape)
print(x_test.shape)

# Convert ground truth arrays to Chainsaw and Background
wake_word_index = all_targets.index(alert)
y_train = np.equal(y_train, wake_word_index).astype('float64')
y_val = np.equal(y_val, wake_word_index).astype('float64')
y_test = np.equal(y_test, wake_word_index).astype('float64')

# Peek at labels after conversion
print(y_val)

# What percentage of 'Chainsaw' appear in validation labels
print(sum(y_val) / len(y_val))
print(1 - sum(y_val) / len(y_val))

# View the dimensions of our input training set
print(x_train.shape)

# Default CNN for TF expects (batch, height, width, channels)
# So we reshape the input tensors with a "color" channel of 1
x_train = x_train.reshape(x_train.shape[0], 
                          x_train.shape[1], 
                          x_train.shape[2], 
                          1)
x_val = x_val.reshape(x_val.shape[0], 
                      x_val.shape[1], 
                      x_val.shape[2], 
                      1)
x_test = x_test.reshape(x_test.shape[0], 
                        x_test.shape[1], 
                        x_test.shape[2], 
                        1)
print(x_train.shape)
print(x_val.shape)
print(x_test.shape)

# Input shape for our CNN is the size of MFCC of 1 sample
sample_shape = x_test.shape[1:]
print(sample_shape)









# Model Architecture

model = models.Sequential()
model.add(layers.Conv2D(32, 
                        (2, 2), 
                        activation='relu',
                        input_shape=sample_shape))
model.add(layers.MaxPooling2D(pool_size=(2, 2)))

model.add(layers.Conv2D(32, (2, 2), activation='relu'))
model.add(layers.MaxPooling2D(pool_size=(2, 2)))

model.add(layers.Conv2D(64, (2, 2), activation='relu'))
model.add(layers.MaxPooling2D(pool_size=(2, 2)))


# Classifier
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dropout(0.1))
model.add(layers.Dense(1, activation='sigmoid'))

# model
model.summary()

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension for monitoring
# %load_ext tensorboard

import datetime
import os

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs

def train_model():
  
  model.compile(optimizer='rmsprop',
                loss='binary_crossentropy',
                metrics=['accuracy'])

  logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
  tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)

  model.fit(x_train, 
                    y_train, 
                    epochs=1, 
                    batch_size=100, 
                    validation_data=(x_val, y_val), 
            callbacks=[tensorboard_callback])

train_model()

# Save the model as a file
models.save_model(model, model_filename)

# See which are 'Chainsaw'
for idx, y in enumerate(y_test):
    if y == 1:
        print(idx)

model = models.load_model(model_filename)
y_predict = model.predict(np.expand_dims(x_test, axis = -1))
probas = np.array(y_predict)
ypred = (probas > 0.5).astype(np.int)
for i in range(0, 20):
    print('Ground Truth:', y_test[i], ' Prediction:', ypred[i])

from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, confusion_matrix, classification_report
print(classification_report(y_test, ypred, target_names=all_targets))

import seaborn as sns
import matplotlib.pyplot as plt  

cm = confusion_matrix(y_test, ypred)
ax= plt.subplot()
sns.heatmap(cm, annot=True, fmt='g', ax=ax);

# labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); 
ax.set_title('LogNet Confusion Matrix'); 
ax.xaxis.set_ticklabels(['Chainsaw', 'Background']); ax.yaxis.set_ticklabels(['Chainsaw', 'Background']);

from tensorflow import lite
from tensorflow.keras import models

# Parameters
keras_model_filename = 'chainsaw_detection_model.h5'
tflite_filename = 'chainsaw_detection_model.tflite'

# Convert model to TF Lite model
model = models.load_model(keras_model_filename)
converter = lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
open(tflite_filename, 'wb').write(tflite_model)